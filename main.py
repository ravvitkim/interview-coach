import os
# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
os.environ['TF_USE_LEGACY_KERAS'] = '0'
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
os.environ['HF_HUB_DISABLE_SYMLINKS'] = '1'

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
import shutil
import uuid
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import librosa
from concurrent.futures import ThreadPoolExecutor
import cv2
import numpy as np
from deepface import DeepFace
import mediapipe as mp
import subprocess
import tempfile

# 1. FastAPI Ïï± Ï¥àÍ∏∞Ìôî
app = FastAPI(title="AI Interviewer API", description="AI Î©¥Ï†ëÍ¥ÄÏùò Î∞±ÏóîÎìú API ÏÑúÎπÑÏä§")

# CORS ÏÑ§Ï†ï
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. Î™®Îç∏ Î∞è ÎèÑÍµ¨ Ï¥àÍ∏∞Ìôî
print("Loading Models... This may take a while.")

device = 0 if torch.cuda.is_available() else -1
print(f"Using device: {'CUDA' if device == 0 else 'CPU'}")

# STT
print("Loading Whisper...")
stt_pipe = pipeline("automatic-speech-recognition", model="openai/whisper-tiny", device=device)

# Í∞êÏ†ï Ïù∏Ïãù (Transformers)
print("Loading Emotion Model...")
emotion_pipe = pipeline("audio-classification", model="superb/wav2vec2-base-superb-er", device=device)

# LLM (ÏÜçÎèÑ Í∞úÏÑ†ÏùÑ ÏúÑÌï¥ 0.5B Î™®Îç∏ ÏÇ¨Ïö©)
print("Loading LLM...")
model_name = "Qwen/Qwen2.5-0.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
llm_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

# Mediapipe
mp_face_mesh = mp.solutions.face_mesh
mp_pose = mp.solutions.pose
LEFT_IRIS = [474, 475, 476, 477]
RIGHT_IRIS = [469, 470, 471, 472]

print("Models Loaded Successfully!")

# 3. Î∂ÑÏÑù Ìï®Ïàò Ï†ïÏùò (app.pyÏóêÏÑú Ïù¥Ïãù)

def analyze_emotions(video_path: str, sample_rate: int = 90) -> dict: # 60 -> 90 (3Ï¥àÏóê 1Î≤à)
    """ÏòÅÏÉÅÏóêÏÑú ÌëúÏ†ï/Í∞êÏ†ï Î∂ÑÏÑù"""
    cap = cv2.VideoCapture(video_path)
    emotions_log = []
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % sample_rate == 0:
            try:
                # Î¶¨ÏÇ¨Ïù¥ÏßïÏúºÎ°ú ÏÜçÎèÑ Ìñ•ÏÉÅ
                small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
                result = DeepFace.analyze(
                    small_frame,
                    actions=['emotion'],
                    enforce_detection=False,
                    silent=True,
                    detector_backend='opencv' # Í∞ÄÎ≤ºÏö¥ Î∞±ÏóîÎìú ÏÇ¨Ïö©
                )
                emotions_log.append(result[0]['emotion'])
            except Exception:
                pass

        frame_count += 1

    cap.release()

    if not emotions_log:
        return {"error": "ÌëúÏ†ï Ïù∏Ïãù Ïã§Ìå®"}

    avg_emotions = {}
    for key in emotions_log[0].keys():
        avg_emotions[key] = round(sum(e[key] for e in emotions_log) / len(emotions_log), 2)

    dominant = max(avg_emotions, key=avg_emotions.get)
    
    emotion_feedback = {
        "happy": "Î∞ùÏùÄ ÌëúÏ†ïÏù¥ Ï¢ãÏäµÎãàÎã§ ‚úÖ",
        "neutral": "Ï∞®Î∂ÑÌïòÍ≥† ÏïàÏ†ïÏ†ÅÏûÖÎãàÎã§ ‚úÖ",
        "sad": "Ï°∞Í∏à Îçî Î∞ùÏùÄ ÌëúÏ†ïÏùÑ ÏßÄÏñ¥Î≥¥ÏÑ∏Ïöî ‚ö†Ô∏è",
        "angry": "ÌëúÏ†ïÏù¥ Îî±Îî±Ìï¥ Î≥¥Ïùº Ïàò ÏûàÏñ¥Ïöî ‚ö†Ô∏è",
        "fear": "Í∏¥Ïû•Ìïú Í≤ÉÏ≤òÎüº Î≥¥Ïó¨Ïöî ‚ö†Ô∏è",
        "surprise": "ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌëúÏ†ïÏùÑ Ïú†ÏßÄÌïòÏÑ∏Ïöî",
        "disgust": "ÌëúÏ†ï Í¥ÄÎ¶¨Í∞Ä ÌïÑÏöîÌï¥Ïöî ‚ö†Ô∏è"
    }

    return {
        "average_emotions": avg_emotions,
        "dominant_emotion": dominant,
        "feedback": emotion_feedback.get(dominant, ""),
        "samples_analyzed": len(emotions_log)
    }

def get_iris_center(landmarks, indices, img_w, img_h):
    points = [(landmarks[i].x * img_w, landmarks[i].y * img_h) for i in indices]
    return np.mean(points, axis=0)

def analyze_gaze(video_path: str, sample_rate: int = 45) -> dict: # 15 -> 45 (1.5Ï¥àÏóê 1Î≤à)
    """ÏãúÏÑ† Ï∂îÏ†Å"""
    cap = cv2.VideoCapture(video_path)
    img_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    img_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    center_x = img_w / 2

    # Î¶¨ÏÇ¨Ïù¥Ïßï ÎπÑÏú® Í≥ÑÏÇ∞ (ÏñºÍµ¥ Ïù∏ÏãùÏù¥ Ïûò ÎêòÎäî ÏÑ†ÏóêÏÑú Ï∂ïÏÜå)
    scale_factor = 0.5 
    
    gaze_results = []
    frame_count = 0

    with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5) as face_mesh:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            if frame_count % sample_rate == 0:
                # MediapipeÎäî ÏõêÎ≥∏ Ìï¥ÏÉÅÎèÑ Ïú†ÏßÄ Í∂åÏû•ÌïòÏßÄÎßå ÎÑàÎ¨¥ ÌÅ¨Î©¥ ÎäêÎ¶º -> Ï†ÅÎãπÌûà Î¶¨ÏÇ¨Ïù¥Ïßï
                small_frame = cv2.resize(frame, (0, 0), fx=scale_factor, fy=scale_factor)
                rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
                results = face_mesh.process(rgb_frame)

                if results.multi_face_landmarks:
                    landmarks = results.multi_face_landmarks[0].landmark
                    left_iris = get_iris_center(landmarks, LEFT_IRIS, img_w, img_h)
                    right_iris = get_iris_center(landmarks, RIGHT_IRIS, img_w, img_h)
                    avg_iris_x = (left_iris[0] + right_iris[0]) / 2
                    
                    is_looking_center = abs(avg_iris_x - center_x) < (img_w * 0.15)
                    gaze_results.append(is_looking_center)

            frame_count += 1

    cap.release()

    if not gaze_results:
        return {"error": "ÏãúÏÑ† Ïù∏Ïãù Ïã§Ìå®"}

    center_ratio = sum(gaze_results) / len(gaze_results) * 100
    
    if center_ratio >= 70:
        feedback = "Ïπ¥Î©îÎùºÎ•º Ïûò ÏùëÏãúÌïòÍ≥† ÏûàÏñ¥Ïöî ‚úÖ"
    elif center_ratio >= 50:
        feedback = "Ïπ¥Î©îÎùºÎ•º Ï°∞Í∏à Îçî Î∞îÎùºÎ¥êÏ£ºÏÑ∏Ïöî ‚ö†Ô∏è"
    else:
        feedback = "ÏãúÏÑ†Ïù¥ ÎßéÏù¥ ÌùîÎì§Î†§Ïöî. Ïπ¥Î©îÎùºÎ•º ÏùëÏãúÌï¥Ï£ºÏÑ∏Ïöî ‚ùå"

    return {
        "center_gaze_ratio": round(center_ratio, 1),
        "feedback": feedback,
        "samples_analyzed": len(gaze_results)
    }

def analyze_posture(video_path: str, sample_rate: int = 60) -> dict: # 15 -> 60 (2Ï¥àÏóê 1Î≤à)
    """ÏûêÏÑ∏ Î∞è Ï†úÏä§Ï≤ò Î∂ÑÏÑù"""
    cap = cv2.VideoCapture(video_path)
    shoulder_positions = []
    hand_movements = []
    prev_wrist_left = None
    prev_wrist_right = None
    frame_count = 0

    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            if frame_count % sample_rate == 0:
                small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
                rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
                results = pose.process(rgb_frame)

                if results.pose_landmarks:
                    landmarks = results.pose_landmarks.landmark
                    
                    # Ïñ¥Íπ®
                    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]
                    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]
                    shoulder_y = (left_shoulder.y + right_shoulder.y) / 2
                    shoulder_positions.append(shoulder_y)

                    # ÏÜêÎ™©
                    left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]
                    right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]

                    if prev_wrist_left and prev_wrist_right:
                        left_move = np.sqrt((left_wrist.x - prev_wrist_left.x)**2 + (left_wrist.y - prev_wrist_left.y)**2)
                        right_move = np.sqrt((right_wrist.x - prev_wrist_right.x)**2 + (right_wrist.y - prev_wrist_right.y)**2)
                        hand_movements.append((left_move + right_move) / 2)

                    prev_wrist_left = left_wrist
                    prev_wrist_right = right_wrist

            frame_count += 1

    cap.release()

    if not shoulder_positions:
        return {"error": "ÏûêÏÑ∏ Ïù∏Ïãù Ïã§Ìå®"}

    shoulder_std = np.std(shoulder_positions)
    stability = max(0, 100 - shoulder_std * 500)
    
    avg_movement = np.mean(hand_movements) if hand_movements else 0
    if avg_movement > 0.05:
        gesture_level = "ÎßéÏùå"
        gesture_feedback = "ÏÜê Ï†úÏä§Ï≤òÍ∞Ä ÎßéÏïÑÏöî. Ï°∞Í∏à Ï§ÑÏó¨Î≥¥ÏÑ∏Ïöî ‚ö†Ô∏è"
    elif avg_movement > 0.02:
        gesture_level = "Ï†ÅÎãπ"
        gesture_feedback = "Ï†ÅÏ†àÌïú Ï†úÏä§Ï≤òÏûÖÎãàÎã§ ‚úÖ"
    else:
        gesture_level = "Ï†ÅÏùå"
        gesture_feedback = "ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï†úÏä§Ï≤òÎ•º Ï∂îÍ∞ÄÌï¥Î≥¥ÏÑ∏Ïöî üí°"

    if stability >= 80:
        posture_feedback = "ÏûêÏÑ∏Í∞Ä ÏïàÏ†ïÏ†ÅÏù¥ÏóêÏöî ‚úÖ"
    elif stability >= 60:
        posture_feedback = "ÏûêÏÑ∏Í∞Ä ÏïΩÍ∞Ñ ÌùîÎì§Î†§Ïöî ‚ö†Ô∏è"
    else:
        posture_feedback = "ÏûêÏÑ∏Î•º Í≥†Ï†ïÌïòÍ≥† ÏïàÏ†ïÍ∞êÏùÑ Ïú†ÏßÄÌïòÏÑ∏Ïöî ‚ùå"

    return {
        "posture_stability": round(stability, 1),
        "posture_feedback": posture_feedback,
        "hand_gesture_level": gesture_level,
        "gesture_feedback": gesture_feedback,
        "samples_analyzed": len(shoulder_positions)
    }

def analyze_interview_video(video_path: str) -> dict:
    """Ï¢ÖÌï© ÎπÑÎîîÏò§ Î∂ÑÏÑù"""
    results = {
        "emotion": analyze_emotions(video_path),
        "gaze": analyze_gaze(video_path),
        "posture": analyze_posture(video_path)
    }

    scores = []
    if "dominant_emotion" in results["emotion"]:
        emotion = results["emotion"]["dominant_emotion"]
        emotion_scores = {"happy": 90, "neutral": 85, "surprise": 70, "sad": 50, "angry": 40, "fear": 45, "disgust": 40}
        scores.append(emotion_scores.get(emotion, 60))

    if "center_gaze_ratio" in results["gaze"]:
        scores.append(results["gaze"]["center_gaze_ratio"])

    if "posture_stability" in results["posture"]:
        scores.append(results["posture"]["posture_stability"])

    overall_score = round(sum(scores) / len(scores), 1) if scores else 0
    
    if overall_score >= 80:
        overall_feedback = "Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú ÌõåÎ•≠Ìïú Î©¥Ï†ë ÌÉúÎèÑÏûÖÎãàÎã§! üéâ"
    elif overall_score >= 60:
        overall_feedback = "Ï¢ãÏùÄ Ìé∏Ïù¥ÏßÄÎßå Í∞úÏÑ†Ìï† Î∂ÄÎ∂ÑÏù¥ ÏûàÏñ¥Ïöî üí™"
    else:
        overall_feedback = "Ïó∞ÏäµÏù¥ Îçî ÌïÑÏöîÌï¥Ïöî. ÌîºÎìúÎ∞±ÏùÑ Ï∞∏Í≥†Ìï¥Ï£ºÏÑ∏Ïöî üìù"

    results["overall"] = {
        "score": overall_score,
        "feedback": overall_feedback
    }
    return results

def extract_audio_from_video(video_path: str) -> str:
    """ffmpegÎ°ú Ïò§ÎîîÏò§ Ï∂îÏ∂ú"""
    try:
        audio_path = tempfile.mktemp(suffix=".wav")
        cmd = [
            "ffmpeg", "-i", video_path,
            "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
            "-y", audio_path
        ]
        # ÏúàÎèÑÏö∞ÏóêÏÑú ÏΩòÏÜî Ï∞Ω Îú®ÏßÄ ÏïäÍ≤å ÏÑ§Ï†ï
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        
        subprocess.run(cmd, capture_output=True, text=True, startupinfo=startupinfo)
        
        if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
            return audio_path
        return None
    except Exception as e:
        print(f"Ïò§ÎîîÏò§ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        return None

# 4. Îç∞Ïù¥ÌÑ∞ Î™®Îç∏ Ï†ïÏùò
class QuestionRequest(BaseModel):
    topic: str
    difficulty: str

class QuestionResponse(BaseModel):
    question: str

# 5. ÏóîÎìúÌè¨Ïù∏Ìä∏ Íµ¨ÌòÑ

@app.post("/generate_question", response_model=QuestionResponse)
async def generate_question_api(req: QuestionRequest):
    if not req.topic:
        raise HTTPException(status_code=400, detail="Topic is required")

    prompt = f"ÎãπÏã†ÏùÄ Î©¥Ï†ëÍ¥ÄÏûÖÎãàÎã§. '{req.topic}' ÏßÅÎ¨¥/Ï£ºÏ†úÏôÄ Í¥ÄÎ†®Îêú Î©¥Ï†ë ÏßàÎ¨∏ÏùÑ Îî± ÌïòÎÇòÎßå ÎçòÏßÄÏÑ∏Ïöî. ÎÇúÏù¥ÎèÑÎäî {req.difficulty}ÏûÖÎãàÎã§. Ï†àÎåÄ ÎãµÎ≥Ä ÏòàÏãúÎÇò ÎπàÏπ∏ Ï±ÑÏö∞Í∏∞ ÌòïÏãùÏúºÎ°ú ÎßåÎì§ÏßÄ ÎßêÍ≥†, ÏßÄÏõêÏûêÏóêÍ≤å Î¨ªÎäî 'ÏùòÎ¨∏Î¨∏' ÌòïÏãùÏùò ÏßàÎ¨∏ ÌïòÎÇòÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî. Ïù∏ÏÇ¨ÎßêÏùÄ ÏÉùÎûµÌï©ÎãàÎã§."
    
    messages = [{"role": "user", "content": prompt}]
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([input_text], return_tensors="pt").to(llm_model.device)

    generated_ids = llm_model.generate(inputs.input_ids, max_new_tokens=100)
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    question_text = response.split("assistant\n")[-1].strip()

    return QuestionResponse(question=question_text)

@app.post("/analyze")
async def analyze_interview_api(
    file: UploadFile = File(None),  # ÎπÑÎîîÏò§ ÎòêÎäî Ïò§ÎîîÏò§ ÌååÏùº
    text_answer: str = Form(None),
    difficulty: str = Form("Ï¥àÍ∏â"),
    question: str = Form(...)
):
    has_file = file is not None
    has_text = text_answer is not None and len(text_answer.strip()) > 0

    if not has_file and not has_text:
        raise HTTPException(status_code=400, detail="File or text answer is required")

    response_data = {
        "stt_result": "",
        "audio_emotion": None,
        "video_analysis": None,
        "feedback": "",
        "best_answer": ""
    }

    temp_file_path = None
    extracted_audio_path = None

    try:
        if has_file:
            # ÌååÏùº Ï†ÄÏû•
            file_ext = file.filename.split(".")[-1]
            temp_file_path = f"temp_{uuid.uuid4()}.{file_ext}"
            with open(temp_file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            
            # ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÎèÑ (Ïã§Ìå® Ïãú Ïò§ÎîîÏò§ ÌååÏùºÎ°ú Í∞ÑÏ£º)
            is_video = False
            try:
                cap = cv2.VideoCapture(temp_file_path)
                if cap.isOpened():
                    # ÌîÑÎ†àÏûÑÏù¥ ÏùΩÌûàÎäîÏßÄ ÌôïÏù∏
                    ret, _ = cap.read()
                    if ret:
                        is_video = True
                cap.release()
            except:
                pass

            target_audio_path = temp_file_path

            if is_video:
                print(">>> [Step 1] ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë...")
                try:
                    video_results = analyze_interview_video(temp_file_path)
                    response_data["video_analysis"] = video_results
                    print(">>> [Step 1] ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏôÑÎ£å")
                    
                    # Ïò§ÎîîÏò§ Ï∂îÏ∂ú
                    print(">>> [Step 2] Ïò§ÎîîÏò§ Ï∂îÏ∂ú ÏãúÏûë...")
                    extracted = extract_audio_from_video(temp_file_path)
                    if extracted:
                        extracted_audio_path = extracted
                        target_audio_path = extracted
                        print(">>> [Step 2] Ïò§ÎîîÏò§ Ï∂îÏ∂ú ÏôÑÎ£å")
                    else:
                        print(">>> [Step 2] Ïò§ÎîîÏò§ Ï∂îÏ∂ú Ïã§Ìå® (ÌååÏùºÏù¥ ÏóÜÍ±∞ÎÇò ÏΩîÎç± Î¨∏Ï†ú)")
                except Exception as e:
                    print(f"FAILED: ÎπÑÎîîÏò§ Î∂ÑÏÑù Ï§ë Ïò§Î•ò: {e}")

            # Ïò§ÎîîÏò§/ÏùåÏÑ± Î∂ÑÏÑù
            if os.path.exists(target_audio_path):
                print(">>> [Step 3] Ïò§ÎîîÏò§(STT/Í∞êÏ†ï) Î∂ÑÏÑù ÏãúÏûë...")
                try:
                    audio_array, _ = librosa.load(target_audio_path, sr=16000)
                    
                    def run_emotion_analysis():
                        print("   -> Í∞êÏ†ï Î∂ÑÏÑù Ï§ë...")
                        results = emotion_pipe(target_audio_path)
                        emotion_probs = {r['label']: float(r['score']) for r in results}
                        top_emotion = results[0]['label'] if results else 'unknown'
                        return top_emotion, emotion_probs

                    with ThreadPoolExecutor() as executor:
                        print("   -> STT Î≥ÄÌôò Ï§ë...")
                        future_stt = executor.submit(lambda: stt_pipe(audio_array)["text"])
                        future_emotion = executor.submit(run_emotion_analysis)
                        
                        response_data["stt_result"] = future_stt.result()
                        print("   -> STT ÏôÑÎ£å")
                        top, probs = future_emotion.result()
                        print("   -> Í∞êÏ†ï Î∂ÑÏÑù ÏôÑÎ£å")
                        response_data["audio_emotion"] = {"top_emotion": top, "probabilities": probs}
                    print(">>> [Step 3] Ïò§ÎîîÏò§ Î∂ÑÏÑù Ï¢ÖÌï© ÏôÑÎ£å")

                except Exception as e:
                    print(f"FAILED: Ïò§ÎîîÏò§ Î∂ÑÏÑù Ïã§Ìå®: {e}")
                    if not response_data["stt_result"] and not is_video:
                         response_data["stt_result"] = "(ÏùåÏÑ± Î∂ÑÏÑù Ïã§Ìå®)"

        else:
            response_data["stt_result"] = text_answer

        # LLM ÌîºÎìúÎ∞±
        print(">>> [Step 4] LLM ÌîºÎìúÎ∞± ÏÉùÏÑ± Ï§ë...")
        top_emo = "ÌÖçÏä§Ìä∏ Î™®Îìú"
        if response_data["audio_emotion"]:
            top_emo = response_data["audio_emotion"]["top_emotion"]
        elif response_data["video_analysis"] and "emotion" in response_data["video_analysis"]:
             # ÎπÑÎîîÏò§ Í∞êÏ†ïÏù¥ ÏûàÏúºÎ©¥ Í∑∏Í≤ÉÎèÑ Ï∞∏Í≥† Í∞ÄÎä•ÌïòÏßÄÎßå Ïó¨Í∏∞ÏÑ† ÏùåÏÑ± Í∞êÏ†ï Ïö∞ÏÑ†ÌïòÍ±∞ÎÇò Î≥ëÍ∏∞
             dominant = response_data["video_analysis"]["emotion"].get("dominant_emotion", "")
             top_emo = f"ÏòÅÏÉÅÌëúÏ†ï:{dominant}"

        prompt_feedback = f"""ÎãπÏã†ÏùÄ Î©¥Ï†ëÍ¥ÄÏûÖÎãàÎã§.
[ÏßàÎ¨∏]: {question}
[ÏßÄÏõêÏûê ÎãµÎ≥Ä]: {response_data["stt_result"]}
[Í∞êÏßÄÎêú Í∞êÏ†ï/ÌÉúÎèÑ]: {top_emo}

ÏßÄÏõêÏûêÏùò ÎãµÎ≥Ä ÎÇ¥Ïö©Í≥º ÌÉúÎèÑ(Í∞êÏ†ï ÏÉÅÌÉú)Ïóê ÎåÄÌï¥ ÌèâÍ∞ÄÌïòÍ≥†, Í∞úÏÑ†Ìï† Ï†êÏùÑ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ï°∞Ïñ∏Ìï¥Ï£ºÏÑ∏Ïöî."""

        messages_f = [{"role": "user", "content": prompt_feedback}]
        input_f = tokenizer.apply_chat_template(messages_f, tokenize=False, add_generation_prompt=True)
        inputs_f = tokenizer([input_f], return_tensors="pt").to(llm_model.device)
        generated_ids_f = llm_model.generate(inputs_f.input_ids, max_new_tokens=400)
        response_data["feedback"] = tokenizer.batch_decode(generated_ids_f, skip_special_tokens=True)[0].split("assistant\n")[-1].strip()
        print(">>> [Step 4] LLM ÌîºÎìúÎ∞± ÏôÑÎ£å")

        # Î™®Î≤î ÎãµÏïà
        print(">>> [Step 5] Î™®Î≤î ÎãµÏïà ÏÉùÏÑ± Ï§ë...")
        prompt_answer = f"""ÎãπÏã†ÏùÄ Î©¥Ï†ëÍ¥ÄÏûÖÎãàÎã§.
[ÏßàÎ¨∏]: {question}
Ïù¥ ÏßàÎ¨∏Ïóê ÎåÄÌï¥ ÏßÄÏõêÏûêÍ∞Ä Ìï† Ïàò ÏûàÎäî Í∞ÄÏû• Ïù¥ÏÉÅÏ†ÅÏù¥Í≥† ÎÖºÎ¶¨Ï†ÅÏù∏ 'ÎßåÏ†êÏßúÎ¶¨ Î™®Î≤î ÎãµÎ≥Ä'ÏùÑ Ïä§ÌÅ¨Î¶ΩÌä∏ ÌòïÌÉúÎ°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî."""
        
        messages_a = [{"role": "user", "content": prompt_answer}]
        input_a = tokenizer.apply_chat_template(messages_a, tokenize=False, add_generation_prompt=True)
        inputs_a = tokenizer([input_a], return_tensors="pt").to(llm_model.device)
        generated_ids_a = llm_model.generate(inputs_a.input_ids, max_new_tokens=400)
        response_data["best_answer"] = tokenizer.batch_decode(generated_ids_a, skip_special_tokens=True)[0].split("assistant\n")[-1].strip()
        print(">>> [Step 5] ÏôÑÎ£å")

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        # ÌååÏùº Ï†ïÎ¶¨
        if temp_file_path and os.path.exists(temp_file_path):
            try: os.remove(temp_file_path)
            except: pass
        if extracted_audio_path and os.path.exists(extracted_audio_path):
             try: os.remove(extracted_audio_path)
             except: pass

    return response_data

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
